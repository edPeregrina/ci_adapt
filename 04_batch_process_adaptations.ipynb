{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to batch process adaptations and analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import configparser\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from ci_adapt_utilities import *\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration with ini file (created running config.py)\n",
    "config_file=r'C:\\repos\\ci_adapt\\config_ci_adapt.ini'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "# Define paths\n",
    "asset_data = config.get('DEFAULT', 'asset_data')\n",
    "data_path = Path(pathlib.Path.home().parts[0]) / 'Data'\n",
    "interim_data_path = data_path / 'interim' / 'collected_flood_runs'\n",
    "\n",
    "# Define costs for different transport modes\n",
    "average_train_load_tons = (896+1344+2160+1344+896+896+1344+1512+896+390)/10 # in Tons per train. Source: Kennisinstituut voor Mobiliteitsbeleid. 2023. Cost Figures for Freight Transport – final report\n",
    "average_train_cost_per_ton_km = (0.014+0.018+0.047+0.045)/4 # in Euros per ton per km. Source: Kennisinstituut voor Mobiliteitsbeleid. 2023. Cost Figures for Freight Transport – final report\n",
    "average_road_cost_per_ton_km = (0.395+0.375+0.246+0.203+0.138+0.153+0.125+0.103+0.122+0.099)/10 # in Euros per ton per km. Source: Kennisinstituut voor Mobiliteitsbeleid. 2023. Cost Figures for Freight Transport – final report\n",
    "\n",
    "# Define dictionaries of return periods and adaptation unit costs\n",
    "return_period_dict = {'_H_': 10,'_M_': 100,'_L_': 200}\n",
    "adaptation_unit_costs = {'fwall': 7408, #considering floodwall in Germany\n",
    "                         'viaduct': 36666, #considering viaduct cost\n",
    "                         'bridge': 40102}  #considering bridge of 10m deck width\n",
    "rp_spec_priority = set_rp_priorities(return_period_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47210 assets loaded.\n",
      "Loaded data from baseline impact assessment\n",
      "Creating virtual graph...\n",
      "Success: only int type values\n"
     ]
    }
   ],
   "source": [
    "# Load data from baseline impact assessment\n",
    "assets_path = data_path / asset_data\n",
    "assets=preprocess_assets(assets_path)\n",
    "\n",
    "# Add buffer to assets to do area intersect and create dictionaries for quicker lookup\n",
    "# buffered_assets = ds.buffer_assets(assets)\n",
    "geom_dict = assets['geometry'].to_dict()\n",
    "\n",
    "print(f\"{len(assets)} assets loaded.\")\n",
    "\n",
    "shortest_paths = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'shortest_paths.pkl', 'rb'))\n",
    "disrupted_edges_by_basin = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'disrupted_edges_by_basin.pkl', 'rb'))\n",
    "graph_r0 = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'graph_0.pkl', 'rb'))\n",
    "disrupted_shortest_paths = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'disrupted_shortest_paths.pkl', 'rb'))\n",
    "event_impacts = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'event_impacts.pkl', 'rb'))\n",
    "full_flood_event=pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'full_flood_event.pkl', 'rb'))\n",
    "all_disrupted_edges = pickle.load(open(data_path / 'interim' / 'indirect_damages' / 'all_disrupted_edges.pkl', 'rb'))\n",
    "collect_output = pickle.load(open(data_path / 'interim' / 'collected_flood_runs' / 'sample_collected_run.pkl', 'rb'))\n",
    "print('Loaded data from baseline impact assessment')\n",
    "graph_v0=create_virtual_graph(graph_r0)\n",
    "graph_v=graph_v0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptations={}\n",
    "adaptations['baseline'] = {'l1_l2_adapt_path': None, 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l1_trib'] = {'l1_l2_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l1_tributary.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l1_trib_M'] = {'l1_l2_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l1_tributary_M.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l1_trib_H'] = {'l1_l2_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l1_tributary_H.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l2_trib'] = {'l1_l2_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l2_tributary.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l3_trib'] = {'l1_l2_adapt_path': None, 'added_links':[(4424116, 219651487), (219651487, 111997047)], 'l4_adapt_path': None}\n",
    "adaptations['l3a_trib'] = {'l1_l2_adapt_path': None, 'added_links':[(4424116, 219651487)], 'l4_adapt_path': None}\n",
    "adaptations['l3b_trib'] = {'l1_l2_adapt_path': None, 'added_links':[(219651487, 111997047)], 'l4_adapt_path': None}\n",
    "adaptations['l4_trib'] = {'l1_l2_adapt_path': None, 'added_links':[], 'l4_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l4_tributary.geojson')}\n",
    "adaptations['l1_rhine'] = {'l1_l2_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l1_rhine.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l1_rhine_M'] = {'l1_l2_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l1_rhine_M.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l1_rhine_H'] = {'l1_l2_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l1_rhine_H.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l2_rhine'] = {'l1_l2_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l2_rhine.geojson'), 'added_links':[], 'l4_adapt_path': None}\n",
    "adaptations['l3_rhine'] = {'l1_l2_adapt_path': None, 'added_links':[(112044105, 110947346)], 'l4_adapt_path': None} \n",
    "# adaptations['l3_rhine'] = {'l1_l2_adapt_path': None, 'added_links':[(112044105, 35861458)], 'l4_adapt_path': None} #35861458 instead of 110947346\n",
    "adaptations['l4_rhine'] = {'l1_l2_adapt_path': None, 'added_links':[], 'l4_adapt_path': Path(r'C:\\Data\\input\\adaptations\\l4_rhine.geojson')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 15 scenarios:\n",
      "-  baseline\n",
      "-  l1_trib\n",
      "-  l1_trib_M\n",
      "-  l1_trib_H\n",
      "-  l2_trib\n",
      "-  l3_trib\n",
      "-  l3a_trib\n",
      "-  l3b_trib\n",
      "-  l4_trib\n",
      "-  l1_rhine\n",
      "-  l1_rhine_M\n",
      "-  l1_rhine_H\n",
      "-  l2_rhine\n",
      "-  l3_rhine\n",
      "-  l4_rhine\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b124a01ffb674b1eb532c716b7e6f345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adaptation runs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptation baseline already processed. Skipping.\n",
      "Adaptation l1_trib already processed. Skipping.\n",
      "Adaptation l1_trib_M already processed. Skipping.\n",
      "Adaptation l1_trib_H already processed. Skipping.\n",
      "Adaptation l2_trib already processed. Skipping.\n",
      "Adaptation l3_trib already processed. Skipping.\n",
      "Adaptation l3a_trib already processed. Skipping.\n",
      "Adaptation l3b_trib already processed. Skipping.\n",
      "Adaptation l4_trib already processed. Skipping.\n",
      "Adaptation l1_rhine already processed. Skipping.\n",
      "Adaptation l1_rhine_M already processed. Skipping.\n",
      "Adaptation l1_rhine_H already processed. Skipping.\n",
      "Adaptation l2_rhine already processed. Skipping.\n",
      "Adaptation l3_rhine already processed. Skipping.\n",
      "Applying adaptation: reduced demand for routes:  [('node_15695', 'node_15775'), ('node_15775', 'node_15695'), ('node_16075', 'node_15695'), ('node_15695', 'node_16075'), ('node_35598', 'node_15695'), ('node_15695', 'node_35598'), ('node_15695', 'node_8204'), ('node_8204', 'node_15695'), ('node_18308', 'node_15695'), ('node_15695', 'node_18308')]\n",
      "Level 4 adaptation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980ca62bdff74b08bf2c7fa0630c6939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing adapted damages by hazard map:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582aaaedee6346f28bdcdc099ae97cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing full flood events:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define empty dictionaries to store adaptation results\n",
    "direct_damages_adapted_dict = {}\n",
    "indirect_damages_adapted_dict = {}\n",
    "indirect_damages_adapted_full_dict = {}\n",
    "adaptation_costs={}\n",
    "adapted_assets_dict = {}\n",
    "\n",
    "# Print adaptations that will be run\n",
    "print(f\"Processing {len(adaptations)} scenarios:\")\n",
    "for adapt_id in adaptations.keys():\n",
    "    print('- ',adapt_id)\n",
    "\n",
    "for adapt_id in tqdm(adaptations.keys(), desc='Adaptation runs', total=len(adaptations)):\n",
    "    adaptations_df_path = data_path / 'interim' / 'adaptations' / f'{adapt_id}_adaptations.csv'\n",
    "\n",
    "    if adaptations_df_path.exists():\n",
    "        print(f\"Adaptation {adapt_id} already processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # reset variables\n",
    "    graph_v=graph_v0.copy()\n",
    "\n",
    "    # Load adaptations dictionary to the relevant variables\n",
    "    l1_l2_adapt_path = adaptations[adapt_id]['l1_l2_adapt_path']\n",
    "    added_links = adaptations[adapt_id]['added_links']  \n",
    "    l4_adapt_path = adaptations[adapt_id]['l4_adapt_path']\n",
    "\n",
    "    # Load adaptation data\n",
    "    if l1_l2_adapt_path is not None:\n",
    "        adapted_area = gpd.read_file(l1_l2_adapt_path).to_crs(3857)\n",
    "    else:\n",
    "        adapted_area = None\n",
    "    if l4_adapt_path is not None:\n",
    "        adapted_route_area = gpd.read_file(l4_adapt_path).to_crs(3857)\n",
    "    else:\n",
    "        adapted_route_area = None\n",
    "\n",
    "    # Apply adaptations\n",
    "    adapted_assets, adaptations_df, demand_reduction_dict, l3_adaptation_costs = apply_adaptations(adapted_area, assets, collect_output, interim_data_path, rp_spec_priority, adaptation_unit_costs, shortest_paths, graph_v, added_links, adapted_route_area)\n",
    "\n",
    "    # Calculate l1 adaptation costs\n",
    "    local_haz_path=data_path/r'Floods\\Germany\\fluvial_undefended\\raw_subsample\\validated_geometries'\n",
    "    l1_adaptation_costs=calculate_l1_costs(local_haz_path, interim_data_path, adapted_area, adaptation_unit_costs, adapted_assets) \n",
    "\n",
    "    # Run adapted damages for individual hazard maps\n",
    "    direct_damages_adapted, indirect_damages_adapted, adaptation_run_full, l2_adaptation_costs, overlay_assets_lists = run_adapted_damages(collect_output, disrupted_edges_by_basin, interim_data_path, assets, geom_dict, adapted_assets, adaptations_df, rp_spec_priority, adaptation_unit_costs, shortest_paths, graph_v, average_train_load_tons, average_train_cost_per_ton_km, average_road_cost_per_ton_km, demand_reduction_dict)\n",
    "\n",
    "    # Run adapted damages for full flood event\n",
    "    indirect_damages_adapted_full = calculate_indirect_dmgs_fullflood(full_flood_event, overlay_assets_lists, adaptation_run_full, assets, all_disrupted_edges, shortest_paths, graph_v, average_train_load_tons, average_train_cost_per_ton_km, average_road_cost_per_ton_km, demand_reduction_dict)\n",
    "\n",
    "\n",
    "    # Fill in missing values in dictionaries\n",
    "    for hazard_map in collect_output.keys():\n",
    "        if direct_damages_adapted[hazard_map]=={}:\n",
    "            direct_damages_adapted[hazard_map]=collect_output[hazard_map]\n",
    "        if indirect_damages_adapted[hazard_map]=={}:\n",
    "            indirect_damages_adapted[hazard_map]=event_impacts[hazard_map] if hazard_map in event_impacts.keys() else 0.0\n",
    "    \n",
    "    # Store results in dictionaries\n",
    "    direct_damages_adapted_dict[adapt_id] = direct_damages_adapted\n",
    "    indirect_damages_adapted_dict[adapt_id] = indirect_damages_adapted\n",
    "    indirect_damages_adapted_full_dict[adapt_id] = indirect_damages_adapted_full\n",
    "    adapted_assets_dict[adapt_id] = adapted_assets\n",
    "    adaptation_costs[adapt_id] = {'l1': l1_adaptation_costs, 'l2': l2_adaptation_costs, 'l3': l3_adaptation_costs}\n",
    "    adaptations_df.to_csv(data_path / 'interim' / 'adaptations' / f'{adapt_id}_adaptations.csv')\n",
    "\n",
    "    # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l4_rhine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'flood_DERP_RW_H_4326_2080410170': {2736: (0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'flood_DERP_RW_H_4326_2080410170': 467036.273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'flood_DERP_RW_H': 55050826.584263116, 'flood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Empty GeoDataFrame\n",
       "Columns: [osm_id, asset, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'l1': None, 'l2': {}, 'l3': {}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            l4_rhine\n",
       "0  {'flood_DERP_RW_H_4326_2080410170': {2736: (0,...\n",
       "1  {'flood_DERP_RW_H_4326_2080410170': 467036.273...\n",
       "2  {'flood_DERP_RW_H': 55050826.584263116, 'flood...\n",
       "3  Empty GeoDataFrame\n",
       "Columns: [osm_id, asset, na...\n",
       "4                   {'l1': None, 'l2': {}, 'l3': {}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report output dataframe\n",
    "output_df = pd.DataFrame.from_dict([direct_damages_adapted_dict, indirect_damages_adapted_dict, indirect_damages_adapted_full_dict, adapted_assets_dict, adaptation_costs])\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for adaptation: l4_rhine\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "# adapt_id= 'l1_trib_test'\n",
    "for adapt_id in adaptations.keys():\n",
    "    if not adapt_id in direct_damages_adapted_dict.keys():\n",
    "        continue\n",
    "    direct_damages_adapted_path = data_path / 'output' / f'adapted_direct_damages_{adapt_id}.pkl'\n",
    "    indirect_damages_adapted_path = data_path / 'output' / f'adapted_indirect_damages_{adapt_id}.pkl'\n",
    "    indirect_damages_adapted_full_path = data_path / 'output' / f'adapted_indirect_damages_full_{adapt_id}.pkl'\n",
    "    # adaptations_df_path = data_path / 'output' / f'adaptations_{adapt_id}.csv'\n",
    "    adapted_assets_path = data_path / 'output' / f'adapted_assets_{adapt_id}.pkl'\n",
    "    adaptation_costs_path = data_path / 'output' / f'adaptation_costs_{adapt_id}.pkl'\n",
    "\n",
    "    with open(direct_damages_adapted_path, 'wb') as f:\n",
    "        pickle.dump(direct_damages_adapted_dict[adapt_id], f)\n",
    "    with open(indirect_damages_adapted_path, 'wb') as f:\n",
    "        pickle.dump(indirect_damages_adapted_dict[adapt_id], f)\n",
    "    with open(indirect_damages_adapted_full_path, 'wb') as f:\n",
    "        pickle.dump(indirect_damages_adapted_full_dict[adapt_id], f)    \n",
    "    with open(adapted_assets_path, 'wb') as f:\n",
    "        pickle.dump(adapted_assets_dict[adapt_id], f)\n",
    "    with open(adaptation_costs_path, 'wb') as f:\n",
    "        pickle.dump(adaptation_costs[adapt_id], f)\n",
    "    print(f'Saved results for adaptation: {adapt_id}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# direct_damages_adapted_path = data_path / 'output' / f'adapted_direct_damages_{adapt_id}.pkl'\n",
    "# indirect_damages_adapted_path = data_path / 'output' / f'adapted_indirect_damages_{adapt_id}.pkl'\n",
    "# indirect_damages_adapted_full_path = data_path / 'output' / f'adapted_indirect_damages_full_{adapt_id}.pkl'\n",
    "# adaptations_df_path = data_path / 'output' / f'adaptations_{adapt_id}.csv'\n",
    "# adapted_assets_path = data_path / 'output' / f'adapted_assets_{adapt_id}.pkl'\n",
    "\n",
    "\n",
    "# with open(direct_damages_adapted_path, 'wb') as f:\n",
    "#     pickle.dump(direct_damages_adapted, f)\n",
    "# with open(indirect_damages_adapted_path, 'wb') as f:\n",
    "#     pickle.dump(indirect_damages_adapted, f)\n",
    "# with open(indirect_damages_adapted_full_path, 'wb') as f:    \n",
    "#     pickle.dump(indirect_damages_adapted_full, f)\n",
    "# adaptations_df.to_csv(adaptations_df_path, index=False)\n",
    "\n",
    "# l3_geometries = {}\n",
    "# if added_links != [] and l1_l2_adapt_path is None and l4_adapt_path is None:\n",
    "#     for u, v, k, attr in graph_v.edges(keys=True, data=True):\n",
    "#         if 'osm_id' not in attr:\n",
    "#             continue\n",
    "#         if 'l3_adaptation' in attr['osm_id']:\n",
    "#             # Ensure the geometry is a valid Shapely geometry object\n",
    "#             geometry = attr['geometry']\n",
    "#             if isinstance(geometry, list) and len(geometry) == 1 and isinstance(geometry[0], shapely.LineString):\n",
    "#                 geometry = geometry[0]\n",
    "#             if isinstance(geometry, shapely.LineString):\n",
    "#                 l3_geometries[(u, v)] = geometry\n",
    "#             else:\n",
    "#                 print(f\"Invalid geometry for edge ({u}, {v}): {geometry}\")\n",
    "\n",
    "#     gdf_l3_edges = gpd.GeoDataFrame.from_dict(l3_geometries, orient='index', columns=['geometry'], geometry='geometry', crs=3857)\n",
    "#     gdf_l3_edges.reset_index(inplace=True)\n",
    "\n",
    "#     gdf_l3_edges = gpd.GeoDataFrame(list(l3_geometries.items()), columns=['edge', 'geometry'], geometry='geometry', crs=3857)\n",
    "#     with open(adapted_assets_path, 'wb') as f:\n",
    "#         pickle.dump(gdf_l3_edges, f)\n",
    "\n",
    "# else:\n",
    "#     with open(adapted_assets_path, 'wb') as f:\n",
    "#         pickle.dump(adapted_assets, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "direct_damages_baseline_sum = {\n",
    "    key: (\n",
    "        sum(v[0] for v in collect_output[key].values()), \n",
    "        sum(v[1] for v in collect_output[key].values())\n",
    "        ) \n",
    "        for key in collect_output if key in collect_output.keys()\n",
    "        }\n",
    "direct_damages_adapted_sum = {\n",
    "    key: (\n",
    "        sum(v[0] for v in direct_damages_adapted[key].values()), \n",
    "        sum(v[1] for v in direct_damages_adapted[key].values())\n",
    "        ) \n",
    "        for key in direct_damages_adapted if key in direct_damages_adapted.keys()\n",
    "        }\n",
    "\n",
    "direct_damages_diff = {\n",
    "    key: (\n",
    "        direct_damages_baseline_sum[key][0] - direct_damages_adapted_sum[key][0], \n",
    "        direct_damages_baseline_sum[key][1] - direct_damages_adapted_sum[key][1]\n",
    "        ) for key in direct_damages_baseline_sum\n",
    "        }\n",
    "\n",
    "direct_damages_diff\n",
    "\n",
    "Visualisations\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "total_damages_adapted={}\n",
    "for hazard_map in direct_damages_adapted.keys():\n",
    "\n",
    "    map_rp_spec = hazard_map.split('_')[-3]\n",
    "\n",
    "    adap_costs=adaptations_df['adaptation_cost']\n",
    "    summed_adaptation_costs = sum(adap_costs)\n",
    "\n",
    "    #direct damages\n",
    "    dd_bl=direct_damages_adapted[hazard_map][0]\n",
    "    summed_dd_bl_lower=sum([v[0] for v in dd_bl.values()])\n",
    "    summed_dd_bl_upper=sum([v[1] for v in dd_bl.values()])\n",
    "    dd_ad=direct_damages_adapted[hazard_map][1]\n",
    "    summed_dd_ad_lower=sum([v[0] for v in dd_ad.values()])\n",
    "    summed_dd_ad_upper=sum([v[1] for v in dd_ad.values()])\n",
    "    \n",
    "    #indirect damages\n",
    "    if hazard_map not in event_impacts.keys():\n",
    "        print(f'{hazard_map} not in event_impacts')\n",
    "        id_bl=0\n",
    "        id_ad=0\n",
    "        id_ad_cleaned=0\n",
    "    else:\n",
    "        id_bl=event_impacts[hazard_map]\n",
    "        id_ad=indirect_damages_adapted[hazard_map]\n",
    "        id_ad_cleaned = 0 if id_ad == 99999999999999 else id_ad\n",
    "\n",
    "    total_damages_adapted[hazard_map]=(map_rp_spec, summed_adaptation_costs, (summed_dd_bl_lower, summed_dd_bl_upper), (summed_dd_ad_lower, summed_dd_ad_upper), id_bl, id_ad_cleaned)\n",
    "    \n",
    "total_damages_adapted_df=pd.DataFrame(total_damages_adapted)\n",
    "total_damages_adapted_df=total_damages_adapted_df.T\n",
    "total_damages_adapted_df.columns=['return_period','summed_adaptation_costs', 'summed_dd_bl', 'summed_dd_ad', 'indirect damage baseline [€]', 'indirect damage adapted [€]']\n",
    "\n",
    "# round and turn to million euros for reporting\n",
    "total_damages_adapted_df_mill=total_damages_adapted_df.copy()\n",
    "total_damages_adapted_df_mill['summed_adaptation_costs [M€]']=total_damages_adapted_df_mill['summed_adaptation_costs']/1e6\n",
    "total_damages_adapted_df_mill['summed_dd_bl [M€]']=total_damages_adapted_df_mill['summed_dd_bl'].apply(lambda x: (x[0]/1e6, x[1]/1e6))\n",
    "total_damages_adapted_df_mill['summed_dd_ad [M€]']=total_damages_adapted_df_mill['summed_dd_ad'].apply(lambda x: (x[0]/1e6, x[1]/1e6))\n",
    "total_damages_adapted_df_mill['indirect damage baseline [M€]']=total_damages_adapted_df_mill['indirect damage baseline [€]']/1e6\n",
    "total_damages_adapted_df_mill['indirect damage adapted [M€]']=total_damages_adapted_df_mill['indirect damage adapted [€]']/1e6\n",
    "total_damages_adapted_df_mill.drop(['summed_adaptation_costs','summed_dd_bl', 'summed_dd_ad', 'indirect damage baseline [€]', 'indirect damage adapted [€]'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "custom_order = ['H', 'M', 'L', 'Unknown']\n",
    "\n",
    "total_damages_adapted_df_mill['return_period'] = pd.Categorical(total_damages_adapted_df_mill['return_period'], \n",
    "                                                                categories=custom_order, ordered=True)\n",
    "sorted_total_damages_adapted_df_mill = total_damages_adapted_df_mill.sort_values(by='return_period', ascending=True)\n",
    "sorted_total_damages_adapted_df_mill\n",
    "shortest_paths_assets={}\n",
    "od_assets=[]\n",
    "o_geoms=[]\n",
    "d_geoms=[]\n",
    "for (o,d), (path, demand) in shortest_paths.items():\n",
    "    od_assets_by_sp=[]\n",
    "    o_geoms.append(graph_v.nodes[o]['geometry'])\n",
    "    d_geoms.append(graph_v.nodes[d]['geometry'])\n",
    "\n",
    "    for i in range(len(path)-1):\n",
    "        x=graph_v.edges[path[i], path[i+1], 0]\n",
    "        od_assets_by_sp.append(x['osm_id'])\n",
    "        od_assets.append(x['osm_id'])\n",
    "\n",
    "    shortest_paths_assets[(o,d)]=(od_assets_by_sp, demand)\n",
    "assets_sps=assets.loc[assets['osm_id'].isin(set(od_assets))].copy()\n",
    "\n",
    "\n",
    "# Repeat for shortest paths under adapted conditions\n",
    "shortest_paths_adapted_assets={}\n",
    "for flood_map, od_dict in disrupted_shortest_paths.items():\n",
    "    for (o,d), (path, demand) in od_dict.items():\n",
    "        od_assets_by_sp_adapted=[]\n",
    "        for i in range(len(path)-1):\n",
    "            x=graph_v.edges[path[i], path[i+1], 0]\n",
    "            od_assets_by_sp_adapted.append(x['osm_id'])\n",
    "        shortest_paths_adapted_assets[flood_map, (o,d)]=(od_assets_by_sp_adapted, demand)\n",
    "\n",
    "\n",
    "df_shortest_paths=pd.DataFrame(shortest_paths_assets).T\n",
    "df_shortest_paths.columns=['path', 'demand']\n",
    "df_shortest_paths\n",
    "\n",
    "\n",
    "df_shortest_paths_adapted=pd.DataFrame(shortest_paths_adapted_assets).T\n",
    "df_shortest_paths_adapted.columns=['path', 'demand']\n",
    "df_shortest_paths_adapted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EXPAND FOR VISUALISATION SCRIPT\n",
    "# Create visualisation for the basin and the discharge points\n",
    "from lonboard import Map, PolygonLayer, PathLayer, BaseLayer, ScatterplotLayer\n",
    "import ast\n",
    "# MIRACA color scheme\n",
    "color_string = config.get('DEFAULT', 'miraca_colors')\n",
    "miraca_colors = ast.literal_eval(color_string)\n",
    "\n",
    "basin_id = 2080430320\n",
    "rp_vis = 'M'\n",
    "overlay_assets = load_baseline_run(f'flood_DERP_RW_{rp_vis}_4326_{basin_id}', interim_data_path, only_overlay=True)\n",
    "    \n",
    "# Set path for basin to add to visualization\n",
    "basin_path = rf'C:\\Data\\Floods\\basins\\hybas_eu_{int(basin_id)}.shp'\n",
    "\n",
    "# Generate the basin layer\n",
    "basin = gpd.read_file(basin_path)\n",
    "layer_basin = PolygonLayer.from_geopandas(basin,\n",
    "    get_fill_color=miraca_colors['grey_100'],\n",
    "    get_line_color=miraca_colors['accent green'], get_line_width=10,\n",
    "    auto_highlight=False,\n",
    "    filled=True, opacity=0.1)\n",
    "\n",
    "# Generate od layer for visualization\n",
    "od_geoms=o_geoms+d_geoms\n",
    "od_geoms_gdf=gpd.GeoDataFrame(geometry=od_geoms).set_crs(3857)\n",
    "layer_od = ScatterplotLayer.from_geopandas(od_geoms_gdf, get_fill_color=miraca_colors['red_danger'], get_radius=200, opacity=0.8, auto_highlight=True)\n",
    "\n",
    "\n",
    "\n",
    "# Set path for the protected area to add to visualization\n",
    "adapt_path = l1_l2_adapt_path\n",
    "if adapt_path is not None:\n",
    "    adapt_area = gpd.read_file(adapt_path)\n",
    "    if len(adapt_area.adapt_level.unique())==1 and adapt_area.adapt_level.unique()[0]==1:\n",
    "        layer_adapted_area = PolygonLayer.from_geopandas(adapt_area,\n",
    "                                                         get_fill_color=miraca_colors['green_800'],\n",
    "                                                         get_line_color=miraca_colors['primary blue'], get_line_width=10,\n",
    "                                                         auto_highlight=False, filled=True, opacity=0.1)\n",
    "    else:\n",
    "        layer_adapted_area = None\n",
    "else:\n",
    "    layer_adapted_area = None\n",
    "\n",
    "# Create layer for assets for visualization\n",
    "layer_assets = PathLayer.from_geopandas(assets.drop(columns=['buffered', 'other_tags']), get_width=80, get_color=miraca_colors['grey_400'], auto_highlight=True, )\n",
    "layer_shortest_path_assets = PathLayer.from_geopandas(assets_sps.drop(columns=['buffered', 'other_tags']), get_width=80, get_color=miraca_colors['black'], auto_highlight=True)\n",
    "affected_assets = [asset_id for asset_id in list(set(overlay_assets.asset.values))]\n",
    "layer_affected_assets = PathLayer.from_geopandas(assets.iloc[affected_assets].drop(columns=['buffered', 'other_tags']), get_width=80, get_color=miraca_colors['red_danger'], auto_highlight=True)\n",
    "\n",
    "try:\n",
    "    layer_protected_assets = PathLayer.from_geopandas(adapted_assets.drop(columns=['buffered', 'other_tags']), get_width=90, get_color=miraca_colors['green_success'], auto_highlight=True)\n",
    "except:\n",
    "    layer_protected_assets = None\n",
    "try:\n",
    "    layer_l3_edges = PathLayer.from_geopandas(gdf_l3_edges, get_width=90, get_color=miraca_colors['green_success'])\n",
    "except:\n",
    "    layer_l3_edges = None\n",
    "layer_assets_raw = [layer_assets, layer_shortest_path_assets, layer_affected_assets, layer_protected_assets, layer_l3_edges]\n",
    "layers_assets = [layer for layer in layer_assets_raw if layer is not None]\n",
    "\n",
    "# Generate flood layers and protection layers for visualization\n",
    "flood_plot_path=rf'Floods\\Germany\\basin_intersections\\DERP_RW_{rp_vis}_4326_hybas_intersections\\flood_DERP_RW_{rp_vis}_4326_{basin_id}.geojson'\n",
    "flood_m = data_path / flood_plot_path\n",
    "flood_gdf=gpd.read_file(flood_m)\n",
    "layers_flood=[]\n",
    "f_area_colors = {1:'blue', 3:'green'}\n",
    "for f_area in flood_gdf.flood_area.unique():\n",
    "    for f_depth in flood_gdf.depth_class.unique():\n",
    "        subset_gdf = flood_gdf[(flood_gdf.depth_class==f_depth) & (flood_gdf.flood_area==f_area)]\n",
    "        if not subset_gdf.empty:\n",
    "            color_key=f'{f_area_colors[f_area]}_{f_depth}00'\n",
    "            layers_flood.append(PolygonLayer.from_geopandas(subset_gdf, \n",
    "                                                            get_fill_color=miraca_colors[color_key], \n",
    "                                                            opacity=0.5, \n",
    "                                                            stroked=False))\n",
    "\n",
    "layers=[]\n",
    "if layer_basin is not None:\n",
    "    layers.append(layer_basin)\n",
    "else:\n",
    "    print('No basin layer')\n",
    "\n",
    "if layer_adapted_area is not None:\n",
    "    layers.append(layer_adapted_area)\n",
    "else:\n",
    "    print('No protected area layer')\n",
    "\n",
    "if layers_flood is not None:\n",
    "    layers.extend(layers_flood)\n",
    "else:\n",
    "    print('No flood layers')\n",
    "\n",
    "if layer_assets is not None:\n",
    "    layers.extend(layers_assets)\n",
    "else:\n",
    "    print('No asset layer')\n",
    "Voyager = 'https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json'\n",
    "\n",
    "if layer_od is not None:\n",
    "    layers.append(layer_od)\n",
    "else:\n",
    "    print('No od layer')\n",
    "m = Map(layers, show_tooltip=True, basemap_style=Voyager, view_state={\"longitude\": 7.91, \"latitude\": 49.91, \"zoom\": 11})\n",
    "\n",
    "m\n",
    "plot_assets=assets.drop(columns=['buffered', 'other_tags']).to_crs(epsg=4326)\n",
    "plot_assets.head(3)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import geopandas as gpd\n",
    "\n",
    "# helper function\n",
    "def zoomlevel_from_deg(deg): #https://stackoverflow.com/questions/30052990/how-to-use-openstreetmap-background-on-matplotlib-basemap\n",
    "    \"Calculate OSM zoom level from a span in degrees.  Adjust +/-1 as desired\"\n",
    "    from numpy import log2, clip, floor\n",
    "    zoomlevel = int(clip(floor(log2(360) - log2(delta)),0,20 ))\n",
    "    return zoomlevel \n",
    "\n",
    "# Convert basin to EPSG:4326 and get the total bounds\n",
    "extent = basin.to_crs(epsg=4326).total_bounds\n",
    "\n",
    "# Add clearance around the basin\n",
    "clearance = 0.5  # 10% clearance around the basin bounds\n",
    "lon_min, lat_min, lon_max, lat_max = extent\n",
    "lon_range = lon_max - lon_min\n",
    "lat_range = lat_max - lat_min\n",
    "\n",
    "lon_min -= clearance * lon_range\n",
    "lat_min -= clearance * lat_range\n",
    "lon_max += clearance * lon_range\n",
    "lat_max += clearance * lat_range\n",
    "\n",
    "# Calculate aspect ratio\n",
    "aspect_ratio = lon_range / lat_range\n",
    "\n",
    "lon_i = lon_min + (lon_max - lon_min) / 2\n",
    "lat_i = lat_min + (lat_max - lat_min) / 2\n",
    "delta = 0.0016 # 0.0012 # 38 to 0.0002 degrees to avoid north pole\n",
    "zoom = zoomlevel_from_deg(delta)-5 # 10 #  0-19 \n",
    "print(f\"Zoom Level: {zoom}\")\n",
    "\n",
    "# Create a plot with the OSM tiles\n",
    "request_osm = cimgt.OSM()\n",
    "\n",
    "fig = plt.figure(figsize=(10 * aspect_ratio, 10))\n",
    "ax = plt.axes(projection=request_osm.crs)\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "ax.add_image(request_osm, zoom, alpha=0.5)  # Adjust zoom level as needed\n",
    "\n",
    "# Plot the basin\n",
    "basin.to_crs(epsg=4326).plot(ax=ax, facecolor='none', edgecolor='grey', linewidth=1, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the assets\n",
    "plot_assets.plot(ax=ax, color='grey', markersize=10, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the flood areas\n",
    "for f_area in flood_gdf.flood_area.unique():\n",
    "    for f_depth in flood_gdf.depth_class.unique():\n",
    "        subset_gdf = flood_gdf[(flood_gdf.depth_class==f_depth) & (flood_gdf.flood_area==f_area)]\n",
    "        if not subset_gdf.empty:\n",
    "            color_key=f'{f_area_colors[f_area]}_{f_depth}00'\n",
    "            subset_gdf.to_crs(epsg=4326).plot(ax=ax, facecolor=miraca_colors[color_key], edgecolor=None, linewidth=2, alpha=0.5, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the shortest paths\n",
    "plot_assets_sps = assets_sps.drop(columns=['buffered', 'other_tags']).to_crs(epsg=4326)\n",
    "plot_assets_sps.plot(ax=ax, color='black', markersize=10, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the affected assets\n",
    "plot_assets_affected = assets.iloc[affected_assets].drop(columns=['buffered', 'other_tags']).to_crs(epsg=4326)\n",
    "plot_assets_affected.plot(ax=ax, color='red', markersize=10, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the OD points\n",
    "od_geoms_gdf.to_crs(epsg=4326).plot(ax=ax, color='blue', markersize=10, transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.gridlines(draw_labels=True)\n",
    "plt.show()\n",
    "import cartopy\n",
    "# Convert basin to EPSG:4326 and get the total bounds\n",
    "extent_new = assets.to_crs(epsg=4326).total_bounds\n",
    "\n",
    "# Add clearance around the basin\n",
    "clearance = 0  # 10% clearance around the basin bounds\n",
    "\n",
    "lon_min_new, lat_min_new, lon_max_new, lat_max_new = extent_new\n",
    "lon_range_new = lon_max_new - lon_min_new\n",
    "lat_range_new = lat_max_new - lat_min_new\n",
    "\n",
    "lon_min_new -= clearance * lon_range_new\n",
    "lat_min_new -= clearance * lat_range_new\n",
    "lon_max_new += clearance * lon_range_new\n",
    "lat_max_new += clearance * lat_range_new\n",
    "\n",
    "# Calculate aspect ratio\n",
    "aspect_ratio_new = lon_range_new / lat_range_new\n",
    "\n",
    "lon_i_new = plot_assets.total_bounds[0] + (plot_assets.total_bounds[2] - plot_assets.total_bounds[0]) / 2\n",
    "lat_i_new = plot_assets.total_bounds[1] + (plot_assets.total_bounds[3] - plot_assets.total_bounds[1]) / 2\n",
    "delta_new = 0.0016 # 0.0012 # 38 to 0.0002 degrees to avoid north pole\n",
    "zoom_new = zoomlevel_from_deg(delta_new)-5 # 10 #  0-19\n",
    "print(f\"Zoom Level: {zoom_new}\")\n",
    "\n",
    "fig = plt.figure(figsize=(10 * aspect_ratio_new, 10))\n",
    "ax = plt.axes(projection=request_osm.crs)\n",
    "ax.set_extent([lon_min_new, lon_max_new, lat_min_new, lat_max_new], crs=ccrs.PlateCarree())\n",
    "ax.add_image(request_osm, zoom_new, alpha=0.5)  # Adjust zoom level as needed\n",
    "\n",
    "# Plot the basin\n",
    "basin.to_crs(epsg=4326).plot(ax=ax, facecolor='none', edgecolor='grey', linewidth=1, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the assets\n",
    "plot_assets.plot(ax=ax, color='grey', markersize=10, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the flood areas\n",
    "for f_area in flood_gdf.flood_area.unique():\n",
    "    for f_depth in flood_gdf.depth_class.unique():\n",
    "        subset_gdf = flood_gdf[(flood_gdf.depth_class==f_depth) & (flood_gdf.flood_area==f_area)]\n",
    "        if not subset_gdf.empty:\n",
    "            color_key=f'{f_area_colors[f_area]}_{f_depth}00'\n",
    "            subset_gdf.to_crs(epsg=4326).plot(ax=ax, facecolor=miraca_colors[color_key], edgecolor=None, linewidth=2, alpha=0.5, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the shortest paths\n",
    "plot_assets_sps = assets_sps.drop(columns=['buffered', 'other_tags']).to_crs(epsg=4326)\n",
    "plot_assets_sps.plot(ax=ax, color='black', markersize=10, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the affected assets\n",
    "plot_assets_affected = assets.iloc[affected_assets].drop(columns=['buffered', 'other_tags']).to_crs(epsg=4326)\n",
    "plot_assets_affected.plot(ax=ax, color='red', markersize=10, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the OD points\n",
    "od_geoms_gdf.to_crs(epsg=4326).plot(ax=ax, color='cyan', markersize=10, transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# add administrative boundaries\n",
    "ax.add_feature(cartopy.feature.BORDERS, linestyle=':')\n",
    "plt.show()\n",
    "#!pip install cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pyproj\n",
    "\n",
    "# Define the transformer to convert from EPSG:3857 to EPSG:4326\n",
    "transformer = pyproj.Transformer.from_crs(\"EPSG:3857\", \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "# Convert coordinates of o_geoms and d_geoms to EPSG:4326\n",
    "o_geoms_4326 = [transformer.transform(geom.x, geom.y) for geom in o_geoms]\n",
    "d_geoms_4326 = [transformer.transform(geom.x, geom.y) for geom in d_geoms]\n",
    "\n",
    "# Create a map with the basin and the origins and destinations\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Plot the basin (assuming basin is in EPSG:3857)\n",
    "# If basin is in EPSG:3857, convert it to EPSG:4326\n",
    "basin = basin.to_crs(epsg=4326)\n",
    "basin.plot(ax=ax, facecolor='none', edgecolor='grey', linewidth=2, alpha = 0.5)\n",
    "\n",
    "# Plot the assets\n",
    "plot_assets.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=2)\n",
    "\n",
    "# Plot the flood areas\n",
    "flood_gdf.plot(ax=ax, facecolor='none', edgecolor='blue', linewidth=1)\n",
    "\n",
    "# Plot the protected area\n",
    "# adapted_area.plot(ax=ax, facecolor='none', edgecolor='green', linewidth=2)\n",
    "\n",
    "# Plot the shortest paths  \n",
    "for (o,d), (path, demand) in shortest_paths.items():\n",
    "    path_coords = [graph_v.nodes[node]['geometry'] for node in path]\n",
    "    ax.plot([geom.x for geom in path_coords], [geom.y for geom in path_coords], 'b-', linewidth=2)\n",
    "\n",
    "# Set the extent of the map based on basin boundaries\n",
    "lon_min, lat_min, lon_max, lat_max = basin.total_bounds\n",
    "\n",
    "\n",
    "# Plot the origins and destinations\n",
    "for lon, lat in o_geoms_4326:\n",
    "    ax.plot(lon, lat, 'ro', markersize=10)\n",
    "for lon, lat in d_geoms_4326:\n",
    "    ax.plot(lon, lat, 'ro', markersize=10)\n",
    "\n",
    "\n",
    "# Calculate the range for longitude and latitude\n",
    "lon_range = lon_max - lon_min\n",
    "lat_range = lat_max - lat_min\n",
    "# Add clearance around the basin\n",
    "clearance = 0.6  # that is 60% of the range between min and max lon and lat on either side\n",
    "# Apply clearance\n",
    "lon_min -= clearance * lon_range\n",
    "lat_min -= clearance * lat_range\n",
    "lon_max += clearance * lon_range\n",
    "lat_max += clearance * lat_range\n",
    "\n",
    "\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add basemap\n",
    "ax.add_feature(cartopy.feature.LAND, edgecolor='black')\n",
    "ax.add_feature(cartopy.feature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cartopy.feature.LAKES, edgecolor='black')\n",
    "ax.add_feature(cartopy.feature.RIVERS)    \n",
    "\n",
    "# Add states and provinces\n",
    "states_provinces = cartopy.feature.NaturalEarthFeature(\n",
    "            category='cultural',  name='admin_1_states_provinces',\n",
    "            scale='10m', facecolor='none')\n",
    "ax.add_feature(states_provinces, edgecolor='black', zorder=10, linestyle = '-', linewidth=0.5)\n",
    "\n",
    "# Add buildings\n",
    "# buildings = cartopy.feature.NaturalEarthFeature(\n",
    "#             category='cultural',  name='buildings',\n",
    "#             scale='10m', facecolor='none')\n",
    "# ax.add_feature(buildings, edgecolor='black', zorder=10, linestyle = '-', linewidth=0.5)\n",
    "\n",
    "ax.gridlines(draw_labels=True)\n",
    "plt.show()\n",
    "m\n",
    "adapted_edges=[(u,v,k) for u,v,k,attr in graph_v.edges(keys=True, data=True) if 'osm_id' in attr and 'l3_adaptation' in attr['osm_id']]\n",
    "print(adapted_edges)\n",
    "# shortest_paths\n",
    "# assets_in_sps2=shortest_paths_assets[('node_164', 'node_10409')]\n",
    "assets_in_sps2=shortest_paths_assets[('node_164', 'node_11238')]\n",
    "# shortest_paths[('node_164', 'node_11238')]\n",
    "# assets_in_sps2\n",
    "# [(u,v) for (u,v), (path, demand) in shortest_paths_assets.items() if '111997044' in path]\n",
    "disrupted_shortest_paths['flood_DERP_RW_L_4326_2080430320'][('node_164', 'node_11238')]==shortest_paths[('node_164', 'node_11238')]\n",
    "print('shortest_paths: ', shortest_paths[('node_164', 'node_11238')])\n",
    "print('disrupted_shortest_paths: ', disrupted_shortest_paths['flood_DERP_RW_L_4326_2080430320'][('node_164', 'node_11238')])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "basins_path = gpd.read_file(data_path / r'Floods\\basins\\hybas_eu_lev01-12_v1c\\hybas_eu_lev08_v1c_valid.shp')\n",
    "regions_path = gpd.read_file(data_path / r'QGIS_data\\rhineland_palatinate.geojson')\n",
    "\n",
    "basin_list_tributaries, basin_list_full_flood = find_basin_lists(basins_path, regions_path)\n",
    "\n",
    "\n",
    "#!pip install seaborn\n",
    "#PLOT INDIRECT DAMAGES AND DIRECT DAMGES FOR EACH HAZARD MAP SORTED BY SUMMED ADAPTATION COSTS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "total_damages_adapted_df_mill=total_damages_adapted_df_mill.sort_values(by='summed_adaptation_costs')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "sns.barplot(x='summed_adaptation_costs', y='hazard_map', data=total_damages_adapted_df_mill, palette='viridis', ax=ax)\n",
    "ax.set_xlabel('Summed adaptation costs [M€]')\n",
    "ax.set_ylabel('Hazard map')\n",
    "plt.show()\n",
    "# indirect damages adapted vs baseline\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "sns.barplot(x='indirect damage adapted [M€]', y='hazard_map', data=total_damages_adapted_df_mill, palette='viridis', ax=ax)\n",
    "ax.set_xlabel('Indirect damage adapted [M€]')\n",
    "ax.set_ylabel('Hazard map')\n",
    "plt.show()\n",
    "\n",
    "#!pip install seaborn\n",
    "#adaptation cost for each basin/hazard map\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "total_damages_adapted_df_mill=total_damages_adapted_df_mill.sort_values(by='summed_adaptation_costs')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "sns.barplot(x='summed_adaptation_costs', y='hazard_map', data=total_damages_adapted_df_mill, palette='viridis', ax=ax)\n",
    "ax.set_xlabel('Summed adaptation costs [M€]')\n",
    "ax.set_ylabel('Hazard map')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci_adapt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
